# ðŸ¥ Medical Insurance Cost Prediction Analysis

## ðŸ“Œ Abstract
This project presents a comparative analysis of machine learning regression techniques applied to the Medical Cost Personal Dataset. The primary objective is to develop a predictive model for estimating individual medical insurance charges based on demographic and lifestyle indicators. We rigorously evaluate **Polynomial Regression** and **Support Vector Regression (SVR)** configurations to identify the optimal modeling approach.

## ðŸ“Š Dataset Overview
The analysis utilizes the **`insurance.csv`** dataset, comprising **1,338** samples with the following features:

| Feature | Type | Description |
| :--- | :--- | :--- |
| **`age`** | Numerical | Age of the primary beneficiary |
| **`sex`** | Categorical | Gender (female, male) |
| **`bmi`** | Numerical | Body mass index ($kg/m^2$) |
| **`children`** | Numerical | Number of children covered by health insurance |
| **`smoker`** | Categorical | Smoking status (yes, no) |
| **`region`** | Categorical | Residential area in the US (northeast, southeast, southwest, northwest) |
| **`charges`** | Numerical | **Target Variable**: Individual medical costs billed by health insurance |

## ðŸ› ï¸ Methodology

### Stage 1: Data Preprocessing & Feature Engineering
*   **Data Cleaning:** Verified zero missing values.
*   **Encoding:** Applied Label Encoding for binary features (`sex`, `smoker`) and One-Hot Encoding for the nominal feature (`region`).
*   **Scaling:** Employed `StandardScaler` to normalize feature distributions, critical for SVR performance.
*   **Feature Selection:** Analyzed correlation coefficients, identifying `smoker`, `bmi`, and `age` as statistically significant predictors.

### Stage 2: Model Architecture
Two distinct families of regression models were implemented and evaluated in separate notebooks:

1.  **Part 1: Polynomial Regression** (`Part1_Polynomial_Regression.ipynb`)
    *   Systematically transformed features to degrees 1 through 5.
    *   Evaluated linearity vs. non-linearity in the feature-target relationship.
    *   **Finding:** Degree 2 provided the optimal bias-variance tradeoff.

2.  **Part 2: Support Vector Regression** (`Part2_SVR.ipynb`)
    *   Leveraged the Kernel Trick to map data into higher-dimensional space.
    *   Evaluated **Linear**, **Polynomial**, and **Radial Basis Function (RBF)** kernels.
    *   **Finding:** The RBF kernel demonstrated superior adaptability to data distribution compared to other kernels.

## ðŸ“ˆ Experimental Results

The models were evaluated using **Mean Squared Error (MSE)**, **Root Mean Squared Error (RMSE)**, and the **Coefficient of Determination ($R^2$)**.

| Model | Configuration | $R^2$ Score | RMSE | Performance Verdict |
| :--- | :---: | :---: | :---: | :--- |
| **Polynomial Regression** | **Degree 2** | **0.8666** | **4,551.13** | ðŸ† **Best Performer** |
| Support Vector Regression | RBF Kernel | 0.8623 | 4,623.88 | ðŸ¥ˆ Runner-up |
| Support Vector Regression | Poly Kernel | 0.8540 | 4,760.23 | ðŸ¥‰ Third Place |
| Linear Regression | Degree 1 | 0.7836 | 5,796.28 | Baseline |
| Support Vector Regression | Linear Kernel | 0.7420 | 6,328.33 | Underfitting |

> **Note:** While higher-degree polynomials (Degree 5) were tested, they exhibited severe overfitting ($R^2 < 0$), confirming that the underlying hypothesis function is quadratic rather than highly complex.

## ðŸ’¡ Conclusion
The analysis concludes that **Polynomial Regression (Degree 2)** is the most robust model for this specific dataset, explaining approximately **86.7%** of the variance in insurance charges. While SVR (RBF) proved highly competitive, the quadratic polynomial expansion captured the interactions between features (likely the non-linear impact of BMI and smoking on age) slightly more effectively and with lower computational complexity.

## ðŸ’» Tech Stack & Requirements
*   **Python 3.x**
*   **Pandas** - Data Manipulation
*   **NumPy** - Numerical Computing
*   **Matplotlib** - Visualization
*   **Scikit-Learn** - Machine Learning Pipelines

---
*Generated by Antigravity AI*
